{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1. Using backpropagation method and the computational graph learned in the class to derive gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta L^r}{\\delta w^l_{ij}}= a^{l-1}_j \\frac{\\delta L^r}{\\delta z^l_i}$  l>1, \n",
    "$\\frac{\\delta L^r}{\\delta w^l_{ij}}= x^{r}_j \\frac{\\delta L^r}{\\delta z^l_i}$ for l=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in these equations for the given 3 layer neural network we get the following equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\delta L^r}{\\delta w^3_{ij}}= a^{2}_j \\frac{\\delta L^r}{\\delta z^3_i}$, \n",
    "\n",
    "$\\frac{\\delta L^r}{\\delta w^2_{ij}}= a^{1}_j \\frac{\\delta L^r}{\\delta z^2_i}$, \n",
    "\n",
    "$\\frac{\\delta L^r}{\\delta w^1_{ij}}= x_j \\frac{\\delta L^r}{\\delta z^1_i}$\n",
    "\n",
    "For the last layer (3), $\\frac{\\delta L^r}{\\delta z^3_i}$ = $\\frac{\\delta L^r}{\\delta y_i} \\frac{\\delta y_i}{\\delta z^3_i}$ = $\\hat{y}_i - y_i * \\frac{e^{z^3_1}e^{z^3_2}}{(e^{z^3_1}+e^{z^3_2})^2}$\n",
    "\n",
    "Let's initialize the weights with random numbers, compute the $\\delta$ 's (errors) and derivatives with respect to the weights using backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$W^1$ is a $2x3$ matrix. $W^2$ is a $3x3$ matrix. $W^3$ is a $3x2$ matrix.\n",
    "X is $1x2$ vector.\n",
    "Lets randomly initialize these matrices to compute $Z^1$ = $W^{1T}X$, $Z^2=W^{2T}a^1$, $Z^3$ = $W^{3T}a^2$. \n",
    "$a^1=\\sigma (Z^1)$, $a^2 = \\sigma(Z^2)$.\n",
    "Let's randomly intialize $W^1$, $W^2$ and $W^3$ weight matrices and compute other quantities in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.rand(2,3)\n",
    "W2 = np.random.rand(3,3)\n",
    "W3 = np.random.rand(3,2)\n",
    "X = np.random.rand(1,2)\n",
    "# dummy one-hot class vector\n",
    "y = np.empty([1,2])\n",
    "y[0,0] = 0.0\n",
    "y[0,1] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00104847  0.00054933  0.00049761]\n",
      " [ 0.00193249  0.00101249  0.00091717]]\n",
      "[[ 0.00504814  0.00498336  0.00348466]\n",
      " [ 0.00582539  0.00575064  0.00402119]\n",
      " [ 0.0049689   0.00490514  0.00342996]]\n",
      "[[ 0.2145273  -0.2145273 ]\n",
      " [ 0.19132985 -0.19132985]\n",
      " [ 0.20871479 -0.20871479]]\n"
     ]
    }
   ],
   "source": [
    "# Forward propogation\n",
    "z1 = X.dot(W1)\n",
    "a1 = sigmoid(z1)\n",
    "z2 = a1.dot(W2)\n",
    "a2 = sigmoid(z2)\n",
    "z3 = a2.dot(W3)\n",
    "exp_scores = np.exp(z3)\n",
    "# softmax\n",
    "probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "# Backpropogation\n",
    "ones = np.ones(exp_scores.shape)\n",
    "# derivative of softmax\n",
    "dsoftmax = ones*(np.prod(exp_scores)/(np.sum(exp_scores)*np.sum(exp_scores)))\n",
    "# derivative of loss w.r.t its inputs\n",
    "dy = dsoftmax*2*(probs-y)\n",
    "# error in the second hidden layer\n",
    "delta3 = sigmoid(z2)*(1-sigmoid(z2))*dy.dot(W3.T)\n",
    "# error in the first hidden layer\n",
    "delta2 = sigmoid(z1)*(1-sigmoid(z1))*delta3.dot(W2.T)\n",
    "\n",
    "# gradients of Loss w.r.t model parameters             \n",
    "dW3 = (a2.T).dot(dy)\n",
    "dW2 = (a1.T).dot(delta3)\n",
    "dW1 = np.dot(X.T, delta2)\n",
    "\n",
    "# dW3, dW2, dW1 represent the gradient of loss w.r.t to W1, W2 and W3 respectively\n",
    "print(dW1) # 2x3\n",
    "print(dW2) # 3x3\n",
    "print(dW3) # 3x2\n",
    "\n",
    "# Z1 = np.matmul(W1.transpose(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2. Implement the stochastic gradient decent (SGD) algorithm learned in the class to train the above neural network.\n",
    "Note: Remove the spaces around #x1#, #x2# and class from train_data.txt and test_data.txt to execute the code without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def sigmoid(z):\n",
    "  return 1 / (1 + np.exp(-1*z))\n",
    "\n",
    "def softmax(z):\n",
    "    return (np.exp(z) / np.sum(np.exp(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to predict an output (0 or 1) and compute accuracy\n",
    "def predict(model, x):\n",
    "    W1, W2, W3 = model['W1'], model['W2'], model['W3']\n",
    "    confusion_matrix=[[0,0],[0,0]]\n",
    "    # Testing the model\n",
    "    accuracy = 0\n",
    "    for i, X in enumerate(test_data, 0):\n",
    "        X = np.reshape(X, (1, 2))\n",
    "        y = np.reshape(test_labels[i, :], (1,2))\n",
    "        z1 = X.dot(W1)\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = a1.dot(W2)\n",
    "        a2 = sigmoid(z2)\n",
    "        z3 = a2.dot(W3)\n",
    "        exp_scores = np.exp(z3)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        idx = np.argmax(probs, axis=1)\n",
    "        accuracy+=y[0, idx]\n",
    "        \n",
    "        ix = idx.astype(bool)\n",
    "        l = y[0].astype(bool)\n",
    "        \n",
    "        if l[0]:\n",
    "            if not ix[0]:\n",
    "                confusion_matrix[0][0] +=1\n",
    "            else:\n",
    "                confusion_matrix[0][1] +=1\n",
    "        elif l[1]:\n",
    "            if ix[0]:\n",
    "                confusion_matrix[1][1] +=1\n",
    "            else:\n",
    "                confusion_matrix[1][0]+=1\n",
    "\n",
    "    return confusion_matrix, (accuracy/test_data.shape[0]*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function learns parameters for the neural network and returns the model.\n",
    "def build_model(train_data, labels, input_dim, hidden_dim, output_dim, epochs=1):\n",
    "    \"\"\"\n",
    "    train_data: training data\n",
    "    labels: training data labels in one-hot vector format\n",
    "    input_dim: 2 in our case\n",
    "    hidden_dim: 3 in our case\n",
    "    output_dim: 2 in our case\n",
    "    epochs: no. of passes through the training data\n",
    "    \"\"\"\n",
    "    # Initialize the parameters to random values. We need to learn these.\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.randn(input_dim, hidden_dim) / np.sqrt(input_dim)\n",
    "    W2 = np.random.randn(hidden_dim, hidden_dim) / np.sqrt(hidden_dim)\n",
    "    W3 = np.random.randn(hidden_dim, output_dim) / np.sqrt(hidden_dim)\n",
    "\n",
    "    # This is what we return at the end\n",
    "    model = {}\n",
    "\n",
    "    # Gradient descent. For each batch...\n",
    "    for epoch in range(0, epochs):\n",
    "        print(\"Training for epoch: {}\".format(epoch+1))\n",
    "        for i, X in enumerate(train_data):\n",
    "\n",
    "            # Forward propagation\n",
    "            X = np.reshape(X, (1, 2))\n",
    "            y = np.reshape(labels[i, :], (1,2))\n",
    "            z1 = X.dot(W1)\n",
    "            a1 = sigmoid(z1)\n",
    "            z2 = a1.dot(W2)\n",
    "            a2 = sigmoid(z2)\n",
    "            z3 = a2.dot(W3)\n",
    "\n",
    "            # softmax\n",
    "            exp_scores = np.exp(z3)\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "            ones = np.ones(exp_scores.shape)\n",
    "\n",
    "            # loss for one example             \n",
    "            loss = np.sum((y-exp_scores)*(y-exp_scores))\n",
    "            \n",
    "            # derivate of softmax w.r.t its inputs z3\n",
    "            dsoftmax = ones*(np.prod(exp_scores)/(np.sum(exp_scores)*np.sum(exp_scores)))\n",
    "\n",
    "            # Backpropogation\n",
    "            dy = dsoftmax*2*(probs-y)\n",
    "            delta3 = sigmoid(z2)*(1-sigmoid(z2))*dy.dot(W3.T)\n",
    "            delta2 = sigmoid(z1)*(1-sigmoid(z1))*delta3.dot(W2.T)\n",
    "\n",
    "            # gradients of Loss w.r.t model parameters             \n",
    "            dW3 = (a2.T).dot(dy)\n",
    "            dW2 = (a1.T).dot(delta3)\n",
    "            dW1 = np.dot(X.T, delta2)\n",
    "\n",
    "            # Gradient descent parameter update\n",
    "            W1 += -epsilon * dW1\n",
    "            W2 += -epsilon * dW2\n",
    "            W3+= -epsilon * dW3\n",
    "\n",
    "    model = { 'W1': W1, 'W2': W2, 'W3': W3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_labels(data):\n",
    "    labels = np.ndarray((data.shape[0], 2), dtype=float)\n",
    "    for i,label in enumerate(data.iloc[:, 2]):\n",
    "        if label == 1:\n",
    "            labels[i,0] = 0\n",
    "            labels[i,1] = 1\n",
    "        elif label == 0:\n",
    "            labels[i,0] = 1\n",
    "            labels[i,1] = 0\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for epoch: 1\n",
      "Confusion Matrix: [[500, 0], [1, 499]]\n",
      "Testing accuracy: 99.9%\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_data.txt') # note remove spaces around the header column in train_data.txt and test_data.txt\n",
    "train_data = train_data.sample(frac=1) # shuffle the training examples\n",
    "train_labels = process_labels(train_data)\n",
    "train_data.drop(['class'], axis = 1, inplace = True)\n",
    "train_data = train_data.values\n",
    "\n",
    "num_examples = len(train_data) # training set size\n",
    "nn_input_dim = 2 # input layer dimensionality\n",
    "nn_output_dim = 2 # output layer dimensionality\n",
    "\n",
    "epsilon = 0.1 # learning rate for gradient descent\n",
    "\n",
    "\n",
    "# Build a model with a 3-dimensional hidden layer\n",
    "model = build_model(train_data, train_labels,  2, 3, 2, epochs=1)\n",
    "\n",
    "# read test data\n",
    "test_data = pd.read_csv('test_data.txt')\n",
    "test_data = test_data.sample(frac=1)\n",
    "test_labels = process_labels(test_data)\n",
    "test_data.drop(['class'], axis = 1, inplace = True)\n",
    "test_data = test_data.values\n",
    "\n",
    "\n",
    "confusion_matrix_1, accuracy = predict(model, test_data)\n",
    "# Test model\n",
    "# printing test accuracy\n",
    "print(\"Confusion Matrix: {}\".format(confusion_matrix_1))\n",
    "print(\"Testing accuracy: {}%\".format(accuracy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Q3. Train the above the neural network on the same training data using the deep learning frameworks. You are free to choose any python package, but your code and results should presented in Jupiter Notebook. I will be using PyTorch to train the above network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 2\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size, bias=False)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.criterion.sizeAverage = False\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        x = F.sigmoid(self.fc3(x))\n",
    "        x = F.softmax(x)\n",
    "        loss = self.criterion(x, target)\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Loading data. Note that the labels need to be converted into one hot vector. If the class is 0, one hot vector will  be [1,0]. If the class is 1, one hot vector representation will be [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_labels(data):\n",
    "    \"\"\"\n",
    "    Converts the third column of the input pandas dataframe into one-hot vectors\n",
    "    \"\"\"\n",
    "    labels = np.ndarray((data.shape[0], 2), dtype=float)\n",
    "    for i,label in enumerate(data.iloc[:, 2]):\n",
    "        if label == 1:\n",
    "            labels[i,0] = 0\n",
    "            labels[i,1] = 1\n",
    "        elif label == 0:\n",
    "            labels[i,0] = 1\n",
    "            labels[i,1] = 0\n",
    "    labels = torch.from_numpy(labels)\n",
    "    labels = labels.float()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data and converting it into appropriate data type for consumption by the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data.txt')\n",
    "train_data = train_data.sample(frac=1)\n",
    "train_labels = process_labels(train_data)\n",
    "train_data.drop(['class'], axis = 1, inplace = True)\n",
    "train_data = train_data.values\n",
    "train_data = torch.from_numpy(train_data)\n",
    "train_data = train_data.float()\n",
    "\n",
    "test_data = pd.read_csv('test_data.txt')\n",
    "test_data = test_data.sample(frac=1)\n",
    "test_labels = process_labels(test_data)\n",
    "test_data.drop(['class'], axis = 1, inplace = True)\n",
    "test_data = test_data.values\n",
    "test_data = torch.from_numpy(test_data)\n",
    "test_data = test_data.float()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to test the model and print accuracy\n",
    "def test_model(model, test_data, test_labels):\n",
    "    # Testing the model\n",
    "    accuracy = 0\n",
    "    confusion_matrix = [[0,0], [0,0]]\n",
    "    for i, data in enumerate(test_data, 0):\n",
    "        labels = test_labels[i, :]\n",
    "        data, labels = Variable(data), Variable(labels)\n",
    "        outputs, _ = net(data, labels)\n",
    "        predicted, idx = torch.max(outputs.data, 0)\n",
    "        accuracy+=labels[idx]\n",
    "        ix = idx.numpy().astype(bool)\n",
    "        l = labels.data.numpy().astype(bool)\n",
    "        if l[0]:\n",
    "            if not ix:\n",
    "                confusion_matrix[0][0] +=1\n",
    "            else:\n",
    "                confusion_matrix[0][1] +=1\n",
    "        elif l[1]:\n",
    "            if ix:\n",
    "                confusion_matrix[1][1] +=1\n",
    "            else:\n",
    "                confusion_matrix[1][0]+=1\n",
    "    return confusion_matrix, (accuracy/test_data.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Initializing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net(2, 3, 2)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training: epoch 0\n",
      "Finished Training: epoch 1\n",
      "Confusion_matrix: [[499, 1], [0, 500]]\n",
      "Test accuracy: Variable containing:\n",
      " 99.9000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs\n",
    "        inputs  = data\n",
    "        labels = train_labels[i, :]\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs, loss = net(inputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Finished Training: epoch {}'.format(epoch))\n",
    "\n",
    "confusion_matrix_2, accuracy = test_model(net, test_data, test_labels)\n",
    "print(\"Confusion_matrix: {}\".format(confusion_matrix))\n",
    "print(\"Test accuracy: {}\".format(accuracy[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. Evaluate models that obtained in question 2 and 3 with the testing data using metrics precision, recall and f-score. Show the details of your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[500, 0], [1, 499]]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_1 # confusion matrix from first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[500, 0], [1, 499]]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_2 # confusion matrix from second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TP = 499/500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision for class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(confusion_matrix):\n",
    "    \"\"\" TP/TP+FP\"\"\"\n",
    "    return (confusion_matrix[1][1]/(confusion_matrix[1][0]+confusion_matrix[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(confusion_matrix):\n",
    "    \"\"\"TP/TP+FN\"\"\"\n",
    "    return (confusion_matrix[1][1]/(confusion_matrix[0][1]+confusion_matrix[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fscore(precision, recall):\n",
    "    return (2*precision*recall/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision1 = precision(confusion_matrix_1)\n",
    "recall1 = recall(confusion_matrix_1)\n",
    "fscore1 = fscore(precision1, recall1)\n",
    "\n",
    "precision2 = precision(confusion_matrix_2)\n",
    "recall2 = recall(confusion_matrix_2)\n",
    "fscore2 = fscore(precision1, recall2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the neural network trained from scratch: precision = 0.998, recall = 1.0 and fscore = 0.998998998998999\n"
     ]
    }
   ],
   "source": [
    "print(\"For the neural network trained from scratch: precision = {}, recall = {} and fscore = {}\".format(precision1, recall1, fscore1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the neural network trained using PyTorch: precision = 0.998, recall = 1.0 and fscore = 0.998998998998999\n"
     ]
    }
   ],
   "source": [
    "print(\"For the neural network trained using PyTorch: precision = {}, recall = {} and fscore = {}\".format(precision2, recall2, fscore2 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
